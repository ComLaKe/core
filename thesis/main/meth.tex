\chapter{Methodology}
Development followed the evolutionary prototyping model: a robust prototype
is built by improving and adding newly understood features~\cite{proto}.
This often took 7--10 days, with requirements, design, implementation
and test suite all being refined.

\section{Requirement Analysis}
In this section, from given context and objectives, we analyzed
the expected system for a set of features and derived a list of use cases.
Supplementary specifications were also added to elaborate
on the nonfuntional requirements.

\subsection{Use-Case Model}\label{uc-model}
As previously introduced, the most basic functions of the data lake core
are \gls{content} uploading and downloading, along with datasets addition
and querying.  A more advanced (and rather powerful) use case is
content extraction, which allows one to fetch only the interested part
of the content, e.g.~extracting rows matching a certain \gls{predicate}
from (semi-)structured data.  Together with logging, the core's use cases
are summarized in figure~\ref{uc}.

\begin{figure}\centering
  \includegraphics[height=0.45\textheight]{figures/uc.eps}
  \caption{Use-case diagram of the core microservice}
  \label{uc}
\end{figure}

\paragraph{Upload content}  This use case allows other microservices
of the data lake to upload a \gls{content}.  Its flow of events is depicted
as follows, where error handling is omitted for brevity, since errors
replace the normal response.
\begin{enumerate}
  \item A \gls{content} is sent to the core microservice.
  \item Core adds the content to the underlying storage
    and register it to the \gls{db}.
  \item Core responds with the \gls{cid} of the added \gls{content}.
\end{enumerate}

\paragraph{Add dataset}  This use case let other services add a dataset:
\begin{enumerate}
  \item A dataset is sent to the core microservice.
  \item Core adds the dataset to the underlying \gls{db}.
  \item Core responds with the \gls{id} of the added dataset.
\end{enumerate}

\paragraph{Find datasets}  This use case allows other services to find
the datasets whose metadata satisfy a given \gls{predicate}:
\begin{enumerate}
  \item A \gls{predicate} is sent to the core microservice.
  \item Core runs a query in the underlying \gls{db} to find matching datasets.
  \item Core responds with a linear collection of metadata,
    each of which satisfying the given \gls{predicate}.
\end{enumerate}

\paragraph{Download content}  In use case, other services fetch a \gls{content}
from the data lake core.
\begin{enumerate}
  \item A \gls{cid} is sent to the core microservice.
  \item Core passes the \gls{cid} to the underlying storage.
  \item Core responds with the respective \gls{content}.
\end{enumerate}

\paragraph{Extract content}  This use case allows other services
to extract a content's parts satisfying a given \gls{predicate}:
\begin{enumerate}
  \item A CID and a \gls{predicate} is sent to the core microservice.
  \item Core iterates the content for matching elements.
  \item Core responds with the extracted elements.
\end{enumerate}

\paragraph{Gather logs}  This use case let system admins study
events occurring in the core microservice for debugging purposes:
\begin{enumerate}
  \item A system admin requests logs from core.
  \item The admin receives the list of past events.
\end{enumerate}

\subsection{Supplementary Specification}
Besides the functionalities specified in the previous section,
the following non-functional requirements were pinned down.

\paragraph{Performance}  Each instance of the data lake core should be able
to respond up to 1000 simultaneous requests, which is approximated
from the number of \gls{usth} researchers and students, every second.
Furthermore, an instance should be able to maintain a high throughput
for large datasets', preferably matching common local bandwidth
(\SI{100}{\mega\bit\per\second} to \SI{1}{\giga\bit\per\second}).

\paragraph{Supportability}  The data lake core and its dependencies must
be able to run on common \gls{os}, including, but not limited to,
\acrshort{gnu}/Linux, Windows and macOS.  While the microservice is likely
to be deployed on the former, it is uncertain if it will be
the future maintainers' \gls{os} of choice for development.  Furthermore,
languages used for implementation and depended systems should be
either familiar or easy to learn.

\paragraph{Licensing}  The resulting software must be released under
a copyleft license, in order to persist digital freedom in scientific research
and promote independence and cooperation in education~\cite{libredu}.

\section{Design}
The design to be presented takes inspiration from several prior arts, such as
Qri and Kylo, especially for high-level ideas.  However, these technologies were
deemed as unsuitable for \gls{usth} due to their goal and scope differences.
Qri is a peer-to-peer application which may have unpredictable performance
for larger but unpopular datasets, and only supports tabular data~\cite{qri}.
Kylo also does not support binary data, while requires substantial resources
for each instance~\cite{kylo}.

\subsection{Architecture}
Considering ICTLab's dynamic budget, the microservice architecture was chosen
by Dr {\selectlanguage{vietnamese}Trần Giang Sơn} for the ease of scaling
individual services out only when in need.  Through his consultancy
and discussions with other interns\footnote{\selectlanguage{vietnamese}Lê
Như Chu Hiệp, Nguyễn Phương Thảo, Nguyễn An Thiết, Trần Minh Hiếu
and Nguyễn Quốc Thông}, it was decided requests from external clients
and most internal services would go through a public \gls{api} for
authentication and authorization before being transformed to comply with
and passed to the core \gls{api}.

With the core service optimized for high \gls{io} performance (high throughput
and low latency), operations of order of growth higher than linear complexity
would be off-loaded to query engines for better horizontal scaling
of compute-intensive tasks.

On the other side, the core service encapsulates \gls{dfs} and \gls{dbms}
and provides a consistent interface for those storages.  Therefore,
the data lake core must also include clients to talk to these outside systems.
For the content extraction use case, we added an \emph{extractor} component
reading from the \gls{fs} and \gls{db}.  The result will be either responded
directly through the core API or cached in the \gls{db}.  The flow directions
of data between previously discussed components are illustrated
in figure~\ref{arch}.

\begin{figure}\centering
  \includegraphics[height=0.45\textheight]{figures/arch.eps}
  \caption{Data lake overall architecture with focus on core's components}
  \label{arch}
\end{figure}

\subsection{Technology Choices}
In a perfect world, choices of technology would be made following
all other design decisions.  However, existing technologies all have
limitations (or at least trade-offs) that we need to be aware of to best
decide on low-level details.

\begin{figure}\centering
  \includegraphics[height=0.45\textheight]{figures/tech.eps}
  \caption{Data lake core high-level architecture}
  \label{tech}
\end{figure}

\subsubsection{Logging}
Events are logged to standard output/error to be picked up
by the logging service provided by the \gls{os} for maximum portability.
On most GNU/Linux distributions, journald does this for every systemd service
and can be configured to upload to any remote endpoint~\cite{journal}
for debugging convenience.

\subsubsection{Communication protocol}
\gls{grpc} was first chosen for its high performance, but was later replaced
by \gls{http} due to the lack of multi-parameter streaming methods.  This is
essential for transporting data together with their metadata and was done
by treating metadata as \gls{http} headers.  As \gls{grpc} uses \gls{http}
under the hood, the change in protocol would not add any overhead.

\subsubsection{Programming languages}
Java\footnote{\url{https://oracle.com/java}} was picked among languages
included in any course from the university's \gls{ict} major, for \gls{jvm}
implementations' performance (comparing to other general-purpose languages'
runtime with garbage-collection~\cite{game}).  The \gls{jvm} also offers
great interoperability with other languages.  For interacting with dynamic data
types, Clojure\footnote{\url{https://clojure.org}} was used to avoid
performing Java reflection.

\subsubsection{Distributed file system}
\gls{hdfs} was initially considered because of Hadoop's popularity
in state-of-the-art data lakes~\cite{lake}, however its lack of presence
in every Unix-like \gls{os}'s repository~\cite{hdfs} (and the reasons behind it)
is an deployment obstacle.

After analyzing several alternatives, \gls{ipfs}\footnote{\url{https://ipfs.io}}
was chosen for its cluster's ability to organically grow or shrink nodes
without any performance interruption thanks to \gls{crdt} consensus~\cite{crdt}.
In addition, the use of Merkle \gls{dag} makes it an append-only storage
with bidirectional mapping between content and \gls{cid} while maintaining
data integrity and eliminating content duplication~\cite{ipfs}.

\subsubsection{Database management system}
Early prototypes used RethinkDB\footnote{\url{https://rethinkdb.com}}
for its embedded query language and dynamic data support.  Due to the lack
of a client with connection pool for \gls{jvm}, though, we had to switched to
PostgreSQL\footnote{\url{https://postgresql.org}}, which also natively support
semi-structured data, to use its \gls{jdbc} driver and improve the performance.

It is worth noting that the connection to both \gls{dfs} and \gls{dbms}
must be abstracted for modularity.  It is entirely possible that in the future
the choices for those will no longer be the most suitable, and the transition
to a fitter technology should be as frictionless as possible.  The interfaces
for the clients of these persistence systems were named following
Java convention as shown in figure~\ref{tech}.

\subsection{Interface}
The \gls{api} was derived from the use cases quite straightforwardly.
Figure~\ref{api} sums up the available endpoints in a logical order.
All appending operations are arranged on the left and the endpoints
on the right are for retrieving the added data.
\begin{figure}
  \includegraphics[width=\textwidth]{figures/api.eps}
  \caption{Core HTTP API endpoints in a common order of access}
  \label{api}
\end{figure}

\begin{itemize}
  \item\verb|POST /dir|: Create an empty directory.
  \item\verb|POST /file|: Add the file to the underlying file system.
  \item\verb|POST /cp|: Copy file or directory inside a directory.
  \item\verb|POST /dataset|: Add the dataset to the lake.
  \item\verb|POST /update|: Add the updated dataset to the lake.
  \item\verb|POST /find|: Find the data according to the given predicate.
  \item\verb|GET /dir/{cid}|: List content of a file system directory.
  \item\verb|GET /file/{cid}|: Stream content from underlying file system.
  \item\verb|GET /schema/{cid}|: Fetch the JSON schema
    of a (semi-)structured content (a file in \gls{json} or \gls{csv} format).
  \item\verb|POST /extract/{cid}|: Extract rows from
    a (semi-)structured content.
\end{itemize}

One notable difference to the \nameref{uc-model} is that operations directly
on contents are separated for files and folders.  Another is the endpoint
for JSON schema: this shall be explained in subsection \ref{future}.

Except for file upload requests and file download responses, all communication
is done in \gls{json}.

\subsection{Database Schema}
The external \gls{db} is used for storing metadata.  For \glspl{content},
other than the \gls{cid}, only the \gls{mime} type is required,
while \emph{extra} fields can be stored under a \verb|jsonb| column---as
a relational database, PostgreSQL cannot provide optional columns.
Each content can then be represented by multiple datasets under
different metadata, or it can be a child of a represented one.

On the other hand, a dataset must be backed by exactly one content,
so its \gls{cid} could be used as a foreign key.  A few generic properties
were believed to be common and useful for indexing, and thus were required:
\emph{description}, \emph{source} and \emph{topics}.  Whilst the first two
might be helpful in a full-text search, \emph{topics} is an array of
keywords which can be used to arrange datasets in a hierarchical view.

To encourage derivation of datasets and support reproducibility, a new entry
is created upon \verb|POST /update| instead of in-place modification.  It is
linked to the \emph{parent} through the \gls{id} and create a \gls{tree}
of datasets that can be reconstructed from a query output.

The described relations were visualized in figure~\ref{db}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/db.eps}
  \caption{PostgreSQL schema for the data lake core}
  \label{db}
\end{figure}

\subsection{Query Language}
Both \verb|POST /find| and \verb|POST /extract/{cid}| require a \gls{predicate}
for searching.  Since the core \gls{api} acts as a bridge between storages
and higher-level services, it would be best to use a language as simple
as possible to express the condition.  In compilers, it is common to use
an \gls{ast} as an intermediate language.

More precisely, an \gls{ast} is not a language, but any tree can
be unambiguously represented by a nested list with a prefix notation.
Its most well-known form is the S-expression, which has been popularized
by the Lisp family of programming languages~\cite{sexpr}.  However, it is
rarely supported by mainstream technologies, while \gls{json} has been
growing in popularity.  Therefore, the \gls{qast} would be expressed
in \gls{json} nested arrays instead of parenthesized lists.  To comply
with \gls{json} syntax, operators must be quoted as a string, for example
the addition operator is \verb|"+"|.  The following are supported:
\begin{itemize}
  \item Arithmetic operators: addition (+), subtraction (-), multiplication (*),
    division (/) and modulo (\%)
  \item Logical operators: \emph{and} (\&), \emph{or} ($\vert$)
    and \emph{not} (!)
  \item Comparison: ==, !=, $>$, $>=$, $<$, $<=$,
    \~ (regular expression matching) and \&\& (intersection)
  \item Field access: current row (\$) and field getter (\verb|.|)
\end{itemize}

An expression may support more than two operands.  This helps flatten the query
and applies to most logical and arithmetic operators (except for modulo)
and the field getter, for example a predicate of \emph{the field \texttt{bar}
of the \texttt{foo} field of the current row inclusively matching `branch' and
`batch'} could be written as \verb|["~", [".", ["$"], "foo", "bar"], "b.*ch"]|.

\section{Implementation}
The implementation is hosted on GitHub under the ComLake
namespace\footnote{\url{https://github.com/ComLake/comlake.core}}.
For the most part, it was derived directly from the design
and use-case sequences.  This section is dedicated for low-level designs
and implementation details of less obvious cases.

\subsection{Error Handling}
In Java, errors are propagated through exceptions.  While they are beneficial
in certain ways, lexical scoping in \verb|try|/\verb|catch| blocks makes it
impossible to put just a variable declaration with initialization in
a \verb|try| block.  This means errors from subsequent statements using
the variable are implicitly caught in the same handling, which can
be unintentional\footnote{One may alternatively declare the variable before
the assignment, but uninitialized variables are subject to a different
class of error.}.

Moreover, information from exceptions are usually meant for debugging,
not to used by the program.  Instead of defining wrapping exceptions
to re-raise everywhere, we created an \verb|Outcome| generic class
whose method's signatures are shown in listing~\ref{outcome}.

An \verb|Outcome<Result, Error>| should be used as a return type,
where \verb|Result| is the type of the original return value, and \verb|Error|
can be anything the caller can analyze, including compound data like sets
and hash tables.

\begin{lstlisting}[label=outcome,caption=Generic outcome type
  for replacing exceptions,language=java]
public class Outcome<Result, Error> {
    public boolean ok;
    public Result result;
    public Error error;
    public static <Result, Error>
      Outcome<Result, Error> pass(Result value);
    public static <Result, Error>
      Outcome<Result, Error> fail(Error value);
}
\end{lstlisting}

\subsection{Input/Output and Concurrency}\label{future}
As the data lake often work with large datasets, it would not be possible
to load an entire request body into memory\footnote{\gls{ipfs} client for Java
needed a patch to enable request streaming but upstream was irresponsive so a
fork has been maintained ever since.}.  Instead, an asynchronous \gls{io} model
was adopted.  In the outermost layer, Aleph\footnote{\url{https://aleph.io}}
and Netty\footnote{\url{https://netty.io}} are used to turn \gls{http}
requests and responds into and from \verb|java.io.InputStream|.  These streams
are only read or immediately written when ready, allowing us to apply
back-pressure from connected storages.

In addition, instead of blocking operations by filling \gls{io} buffer,
this concurrency model switches to the remaining tasks.  Multiple tasks
are executed simultaneously in one thread so fewer threads need to be created,
resulting in lower overhead and better performance.

Such thread pools are also utilized for metadata discovery, such as
schema inferring.  When a \gls{content} is uploaded, its metadata extractor job
is added to the pool and run in parallel with the \gls{http} handler.
While this does not matches the definition of a data lake (schema on read),
it allows such metadata to present in the dataset search (\verb|POST /find|)
without the client explicitly requesting for each entry.  That being said,
in order to extract \emph{data}, the client needs to be aware of the schema
and should not be pooling on the search endpoint.  Hence,
\verb|GET /schema/{cid}| exists as a synchronization point.

Moreover, as it is possible for the schema to be requested before the scheduled
inferring job finishes and duplicate the task, such race condition is minimized
by memoizing the future.  In Clojure, this was done as shown
in listing~\ref{literal}.
\begin{lstlisting}[label=literal,caption=Constructing
  a metadata extractor,language=lisp]
(defn metadata-extractor [fs db]
  (memoize (fn [cid mime] (future ...))))
\end{lstlisting}

\subsection{Metadata Extraction}
From uploaded content, certain metadata can be extracted.  For multimedia,
these are directly embeded in the files, such as Exif for images
and audio recordings (in JPEG, TIFF, WAV or PNG formats~\cite{exif}).
These metadata can later be used for dataset indexing and searching.

For strutured or semi-structured data, a schema could be inferred from
the content, which can later be used to extract specific rows or join
with other data, as seen with \verb|qri sql|~\cite{qricli}.  We stored
these schemata as JSON schema for interopability, and effectively treats
structured formats like \gls{csv} as semi-structured.

\gls{csv} schema were then inferred by pattern matching each value.
In the core service this was done by passing the data to the higher-order
function \verb|reduce|, along with a function \verb|and|'ing the previously
detected type with the result of (regular expression) pattern matching,
falling back to \verb|string|.  Conceptually it would be similar for \gls{json},
but we used a library\footnote{Clojure \gls{json} Schema Validator \& Generator:
\url{https://github.com/luposlip/json-schema}}.

One other semi-structured file format is \gls{xml}.  However, as its elements'
attributes and contents cannot be directly mapped to \gls{json} key-value
objects, it is not possible to generate a JSON schema for a \gls{xml} file.

\subsection{Query Transformations}
As the \gls{qast} is used for both \verb|POST /extract/{cid}|
and \verb|POST /find|, it is respectively translated into \gls{jvm}
and \gls{sql} for processing.  First, the original \gls{json} request body
is converted to a Clojure \verb|vector|.  Its first element is then looked up
in the operator table and the rest are recursively applied to the found function
\verb|op| if the number of arguments is valid.

When translate to \gls{sql}, in \verb|op| is simply string concatenation
with the operator strings.  Since these expressions are in-fix, it is necessary
to wrap parentheses around them.  Strings and arrays also need special
formatting to be recognized by PostgreSQL when the result is used
as a \verb|WHERE| clause.

To extract content, it is slightly more complex: the result has to be
a Clojure function \verb|pred| to be passed to the higher-order \verb|filter|
along with a collection of Clojure \verb|map|.  Let \verb|row| be
a \verb|map|, \verb|pred| would have the signature \verb|(fn [row] ...)|.
Because \verb|pred| is created from \verb|op|, the latter must be of the form
\verb|(fn [args] (fn [row] ...))|.  As operators are applied recursively,
\verb|args| is a collections of \verb|pred|, or each one in \verb|args|
must be called with row: \verb|(map #(% row) args)|.  Combining all these,
a operator mapping for normal functions could be implemented as seen in
listing~\ref{anon}\footnote{The final implementation involved some additional
null-checking.}.
\begin{lstlisting}[label=anon,caption=Constructing a function returning
  another one lazily applying given the given one,language=lisp]
(defn mkfn [func]
  (fn [args]
    (fn [row] (apply func (map #(% row) args)))))
\end{lstlisting}

For example, \verb|(mkfn +)| is an \verb|op| for addition.  Macros, however,
cannot be passed to \verb|mkfn| so their more generic functional equivalent
was used instead, like \verb|every? identity| replacing \verb|apply and|.
After some preprocessing, supported (semi-)structured data (\gls{json}
and \gls{csv}) are filtered by the same \verb|pred|.

With the same \gls{qast} being used for PostgreSQL, \gls{json} and \gls{csv},
query polymorphism has been achieved, which simplifies and unifies
the \gls{api}.

\subsection{Configuration Parsing}
For the ease of deployment, settings for connection to PostgreSQL
and \gls{ipfs} are read from configuration files instead of being hard-coded.
TOML was chosen as the format and configurations are fallen back
user to system configuration file and finally hard-coded defaults.
Their locations on different \gls{os}'s are determined by the library
AppDirs\footnote{\url{https://github.com/harawata/appdirs}}.

\section{Quality Assurance}
To assure the correctness of the implementations, multiple testing methods
are used.  The automated test suite were written as a mixture of
both integration and unit tests, aiming for at least \SI{90}{\percent}
line coverage.  They were run locally as well as on SourceHut \gls{ci}
service\footnote{\url{https://builds.sr.ht/~cnx/comlake.core}}
to test the deployment of depending services as well.  An instance
was also set up on ICTLab with the help and manual testing
of {\selectlanguage{vietnamese}Lê Như Chu Hiệp and Nguyễn Phương Thảo}.
