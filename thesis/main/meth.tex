\chapter{Methodology}
Development followed the evolutionary prototyping model: a robust prototype
is built by improving and adding newly understood features~\cite{proto}.
This often took 7--10 days, with requirements, design, implementation
and test suite all being refined.

\section{Requirement Analysis}
In this section, from given context and objectives, we analyzed
the expected system for a set of features and derived a list of use cases.
Supplementary specifications were also added to elaborate
on the nonfuntional requirements.

\subsection{Use-Case Model}\label{uc-model}
As previously introduced, the most basic functions of the data lake core
are content uploading and downloading, along with datasets addition
and querying.  A more advanced (and rather powerful) use case is
content extraction, which allows one to fetch only the interested part
of the content, e.g.~extracting rows matching a certain \gls{predicate}
from (semi-)structured data.  Together with logging, the core's use cases
are summarized in figure~\ref{uc}.

\begin{figure}\centering
  \includegraphics[height=0.45\textheight]{figures/uc.eps}
  \caption{Use-case diagram of the core microservice}
  \label{uc}
\end{figure}

\paragraph{Upload content}  This use case allows other microservices
of the data lake to upload a \gls{content}.  Its flow of events is depicted
as follows, where error handling is omitted for brevity, since all errors,
if occur, replace the normal response.
\begin{enumerate}
  \item A \gls{content} is sent to the core microservice.
  \item Core adds the content to the underlying storage
    and register it to the \gls{db}.
  \item Core responds with the \gls{cid} of the added \gls{content}.
\end{enumerate}

\paragraph{Add dataset}  This use case let other services add a dataset:
\begin{enumerate}
  \item A dataset is sent to the core microservice.
  \item Core adds the dataset to the underlying \gls{db}.
  \item Core responds with the \gls{id} of the added dataset.
\end{enumerate}

\paragraph{Find datasets}  This use case allows other services to find
the datasets whose metadata satisfy a given \gls{predicate}:
\begin{enumerate}
  \item A \gls{predicate} is sent to the core microservice.
  \item Core runs a query in the underlying \gls{db} to find matching datasets.
  \item Core responds with a linear collection of metadata,
    each of which satisfying the given \gls{predicate}.
\end{enumerate}

\paragraph{Download content}  In use case, other services fetch a \gls{content}
from the data lake core.
\begin{enumerate}
  \item A \gls{cid} is sent to the core microservice.
  \item Core passes the \gls{cid} to the underlying storage.
  \item Core responds with the respective \gls{content}.
\end{enumerate}

\paragraph{Extract content}  This use case allows other services
to extract a content's parts satisfying a given \gls{predicate}:
\begin{enumerate}
  \item A CID and a \gls{predicate} is sent to the core microservice.
  \item Core iterates the content for matching elements.
  \item Core responds with the extracted elements.
\end{enumerate}

\paragraph{Gather logs}  This use case let system admins study
events occurring in the core microservice for debugging purposes:
\begin{enumerate}
  \item A system admin requests logs from core.
  \item The admin receives the list of past events.
\end{enumerate}

\subsection{Supplementary Specification}
Besides the functionalities specified in the previous section,
the following non-functional requirements were pinned down.

\paragraph{Performance}  Each instance of the data lake core should be able
to respond up to 1000 simultaneous requests, which is approximated
from the number of \gls{usth} researchers and students, every second.
Furthermore, an instance should be able to maintain a high throughput
for large datasets', preferably matching common local bandwidth
(\SI{100}{\mega\bit\per\second} to \SI{1}{\giga\bit\per\second}).

\paragraph{Supportability}  The data lake core and its dependencies must
be able to run on common \gls{os}, including, but not limited to,
\acrshort{gnu}/Linux, Windows and macOS.  While the microservice is likely
to be deployed on \acrshort{gnu}/Linux, it is uncertain if it will be
the future maintainers' \gls{os} of choice for development.  Furthermore,
languages used for implementation and depended systems should be
either familiar or easy to learn.

\paragraph{Licensing}  The resulting software must be released under
a copyleft license, in order to persist digital freedom in scientific research
and promote independence and cooperation in education~\cite{libredu}.

\section{Design}
The design to be presented takes inspiration from several prior arts, such as
Qri and Kylo, especially for high-level ideas.  However, these technologies
were deemed unsuitable for \gls{usth} due to their goal and scope differences.
Qri is a peer-to-peer application which may have unpredictable performance
for larger but unpopular datasets, and only supports tabular data~\cite{qri}.
Kylo also does not support binary data, while requires substantial resources
for each instance~\cite{kylo}.

\subsection{Architecture}
Considering ICTLab's dynamic budget, the microservice architecture was chosen
by Dr {\selectlanguage{vietnamese}Trần Giang Sơn} for the ease of scaling
individual services out only when in need.  Through his consultancy
and discussions with other interns\footnote{\selectlanguage{vietnamese}Lê
Như Chu Hiệp, Nguyễn Phương Thảo, Nguyễn An Thiết, Trần Minh Hiếu
and Nguyễn Quốc Thông}, it was decided requests from external clients
and most internal services would go through a public \gls{api} for
authentication and authorization before being transformed to comply with
and passed to the core \gls{api}.

With the core service optimized for high \gls{io} performance (high throughput
and low latency), operations of order of growth higher than linear complexity
would be off-loaded to query engines for better horizontal scaling
of compute-intensive tasks.

On the other side, the core service encapsulates \gls{dfs} and \gls{dbms}
and provides a consistent interface for those storages.  Therefore,
the data lake core must also include clients to talk to these outside systems.
For the content extraction use case, we added an \emph{extractor} component
reading from the \gls{fs} and \gls{db}.  The result will be either responded
directly through the core API or cached in the \gls{db}.  The flow directions
of data between previously discussed components are illustrated
in figure~\ref{arch}.

\begin{figure}\centering
  \includegraphics[height=0.45\textheight]{figures/arch.eps}
  \caption{Data lake overall architecture with focus on core's components}
  \label{arch}
\end{figure}

\subsection{Technology Choices}
In a perfect world, choices of technology would be made following
all other design decisions.  However, existing technologies all have
limitations (or at least trade-offs) that we need to be aware of to best
decide on low-level details.

\begin{figure}\centering
  \includegraphics[height=0.45\textheight]{figures/tech.eps}
  \caption{Data lake core high-level architecture}
  \label{tech}
\end{figure}

\subsubsection{Logging}
Events are logged to standard output/error to be picked up
by the logging service provided by the \gls{os} for maximum portability.
On most GNU/Linux distributions, journald does this for every systemd service
and can be configured to upload to any remote endpoint~\cite{journal}
for debugging convenience.

\subsubsection{Communication protocol}
\gls{grpc} was first chosen for its high performance, but was later replaced
by \gls{http} due to the lack of multi-parameter streaming methods.  This is
essential for transporting data together with their metadata and was done
by treating metadata as \gls{http} headers.  As \gls{grpc} uses \gls{http}
under the hood, the change in protocol would not add any overhead.

\subsubsection{Programming languages}
Java\footnote{\url{https://oracle.com/java}} was picked among languages
included in any course from the university's \gls{ict} major, for \gls{jvm}
implementations' performance (comparing to other general-purpose languages'
runtime with garbage-collection~\cite{game}).  The \gls{jvm} also offers
great interoperability with other languages.  For interacting with dynamic data
types, Clojure\footnote{\url{https://clojure.org}} was used to avoid
performing Java reflection.

\subsubsection{Distributed file system}
\gls{hdfs} was initially considered because of Hadoop's popularity
in state-of-the-art data lakes~\cite{lake}, however its lack of presence
in every Unix-like \gls{os}'s repository~\cite{hdfs} (and the reasons behind it)
is an deployment obstacle.

After analyzing several alternatives, \gls{ipfs}\footnote{\url{https://ipfs.io}}
was chosen for its cluster's ability to organically grow or shrink nodes
without any performance interruption thanks to \gls{crdt} consensus~\cite{crdt}.  In addition, the use of Merkle directed acyclic graphs makes it an append-only
storage with bidirectional mapping between content and \gls{cid}
while maintaining data integrity~\cite{ipfs}.

\subsubsection{Database management system}
Early prototypes used RethinkDB\footnote{\url{https://rethinkdb.com}}
for its embedded query language and dynamic data support.  Due to the lack
of a client with connection pool for \gls{jvm}, though, we had to switched
to PostgreSQL, which also natively support semi-structured data, to use
its \gls{jdbc} driver and improve the performance.

It is worth noting that the connection to both \gls{dfs} and \gls{dbms}
must be abstracted for modularity.  It is entirely possible that in the future
the choices for those will no longer be the most suitable, and the transition
to a fitter technology should be as frictionless as possible.  The interfaces
for the clients of these persistency systems were named following
Java convention as shown in figure~\ref{tech}.

\subsection{Interface}
The \gls{api} was derived from the use cases quite straightforwardly.
Figure~\ref{api} sums up the available endpoints in a logical order.
All appending operations are arranged on the left and the endpoints
on the right are for retrieving the added data.
\begin{figure}
  \includegraphics[width=\textwidth]{figures/api.eps}
  \caption{Core HTTP API endpoints in a common order of access}
  \label{api}
\end{figure}

\begin{itemize}
  \item\verb|POST /dir|: Create an empty directory.
  \item\verb|POST /file|: Add the file to the underlying file system.
  \item\verb|POST /cp|: Copy file or directory inside a directory.
  \item\verb|POST /dataset|: Add the dataset to the lake.
  \item\verb|POST /update|: Add the updated dataset to the lake.
  \item\verb|POST /find|: Find the data according to the given predicate.
  \item\verb|GET /dir/{cid}|: List content of a file system directory.
  \item\verb|GET /file/{cid}|: Stream content from underlying file system.
  \item\verb|GET /schema/{cid}|: Fetch the JSON schema
    of a (semi-)structured content (a file in \gls{json} or \gls{csv} format).
  \item\verb|POST /extract/{cid}|: Extract rows from
    a (semi-)structured content.
\end{itemize}

One notable difference to the \nameref{uc-model} is that operations directly
on contents are separated for files and folders.  Another is the endpoint
for JSON schema: this shall be explained in subsection \ref{future}.

\subsection{Database Schema}
\begin{figure}
  \includegraphics[width=\textwidth]{figures/db.eps}
  \caption{Database schema}
  \label{db}
\end{figure}

\subsection{Query Abstract Syntax Tree}

\section{Implementation}

\subsection{Error Handling}

\subsection{Input/Output Handling}

\subsection{Query Transformations}

\subsection{Concurrency}\label{future}

\subsection{Configuration Parsing}

\section{Quality Assurance}
