\chapter{Evaluation}

\section{Results}
Quantity-wise, the internship has delivered
all expected outcomes and functional requirements.  In summary,
requirement analysis, architecture, design and implementation
of ComLake core were completed.  The \gls{api} was sufficiently
documented\footnote{\url{https://comlake.github.io/comlake.core/api.html}}
for other components to integrate with.

Speaking of performance, the number of connection a core instance could serve
has been in the order of thousands since the transitioning from RethinkDB
to PostgreSQL, as shown in table~\ref{rdbpsql}.  Note that the drop in
the base (control) operation in comlake.core 0.2.0 is likely due to
the introduction of logging.
\begin{table}\centering
  \caption{Performance in requests per second between comlake.core 0.1.1
  and 0.2.0 using RethinkDB and PostgreSQL as \gls{db} back-end respectively,
  benchmarked using wrk on a Intel\textsuperscript{\tiny\textregistered}
  Core\textsuperscript{\tiny\texttrademark} i5-8250U \acrshort{cpu} and
  \SI{8}{\giga\byte} of memory}
  \begin{tabular}{l r r}
    \toprule
    Operation & comlake.core 0.1.1 & comlake.core 0.2.0\\
    \midrule
    File upload \emph{and} \gls{db} insert & 325.40 & 357.28\\
    \gls{db} look-up & 121.01 & 5575.89\\
    Download file & 8217.39 & 6238.30\\
    No-operation (control) & 80994.41 & 29788.50\\
    \bottomrule
  \end{tabular}
  \label{rdbpsql}
\end{table}

In addition to the relatively low latency, ComLake core's throughput
approaches that of \gls{ipfs}, when compared to its \gls{cli} client.
This was confirmed by running the commands in listing~\ref{add} against
a same file of size \SI{2.4}{\giga\byte}.  The result is demonstrated
in table~\ref{throughput}.  \gls{ipfs} performs deduplication passively,
so an reupload of the same file costs fewer operations than the first time.
\begin{lstlisting}[label=add,caption=Adding Cruella.mkv
  through \gls{ipfs} \gls{cli} client and ComLake core,language=sh]
# Uploading
ipfs add -q Cruella.mkv > cid
curl -s -X POST $netloc/file \
  -H Content-Type:video/x-matroska -T Cruella.mkv
# Downloading
ipfs cat $(cat cid) > /dev/null
curl $netloc/file/$(cat cid) > /dev/null
\end{lstlisting}

\begin{table}\centering
  \caption{Performance between go-ipfs \gls{cli} client and comlake.core,
  adding a \SI{2.4}{\giga\byte} to a local \gls{ipfs} node
  on a SK hynix PC401 NVMe solid-state drive}
  \begin{tabular}{l r r}
    \toprule
    Scenario & \gls{ipfs} \gls{cli} & ComLake core\\
    \midrule
    Initial upload & \SI{65}{\mega\byte\per\second}
                   & \SI{61}{\mega\byte\per\second}\\
    Reupload & \SI{228}{\mega\byte\per\second}
             & \SI{199}{\mega\byte\per\second}\\
    Download & \SI{193}{\mega\byte\per\second}
             & \SI{187}{\mega\byte\per\second}\\
    \bottomrule
  \end{tabular}
  \label{throughput}
\end{table}

At the end of the internship period, the codebase had a modest size of under
1200 lines of code (excluding all tests and documentation), over a half which
was in Java and the remaining was in Clojure.  The test suite on the other hand
was written entirely in Clojure.  It contained 28 assertions in total
and covered over \SI{98}{\percent} \emph{of the Clojure code}.  Together with
the documentation, they were released under the \gls{agplv3}.

\begin{lstlisting}[label=extract,caption=Example request and response
  of data extraction,language=sh]
curl -s -X POST $netloc/extract/$cid \
  --data '["~",[".",["$"],"country_name"],"Vie.*"]' \
  -H Content-Type:application/json | jq
[
  {
    "country_code": "VNM",
    "country_name": "Vietnam",
    "indicator_name": "Population, total",
    "year_1969": 42307146,
    ...
  }
]
\end{lstlisting}

Both the test suite and documentation covers every endpoint with concrete
examples to be reproduced via common \gls{http} tools, with one case
in point shown in listing~\ref{extract}, where \verb|$cid| identifies
a \gls{csv} file.

Since the first release, eight tags have been published in a course
of two months, alternating between adding new features and delivering
bug fixes.  Despite intensive testing, we were still lacking the confidence
to bump to a non-zero major version number and warrant a stable interface:
the core \gls{api} changed still slightly every other commit.

\section{Discussion}
In this section, we will be discussing the benefits and disadvantages
of notable choices from the observed results.  The topics will range
from decisions on languages to concurrency model and licensing.

\subsection{Polyglot Development}
One notable aspect of the development process is the interoperation of Java
and Clojure.  While it was seamless to call from one language to another,
some tooling of one often encountered troubles working with the other,
for instance hot loading plugins for Java and Clojure failed to work together,
thus Clojure code had to be manually reloaded and the \gls{http} server
has to be restarted frequently.  One other drawback was the lack of a coverage
measuring tool for both languages, one for Clojure\footnote{Cloverage:
\url{https://github.com/cloverage/cloverage}} was used because it was used
to implement most of the non-trivial data transformation, plus the \gls{http}
routing for Aleph.  This made the reported figure less meaningful; we had
to manually check if Java methods are tested, especially on branching points.

Originally, Java was dictated as the sole language for implementation
for future maintenance by other \gls{usth} students and staff.  Clojure
was added for first-class functions which were used extensively in constructing
\gls{jvm}-native predicates for data extraction.  \Glspl{closure} were also
really handy in this case such as in listing~\ref{anon} and for providing
\gls{fs} and \gls{db} clients explicitly to a memoized function
in listing~\ref{literal} (in contrast to singletons or global variables).
While \gls{homoiconicity} was not utilized directly, Clojure's builtin types
allowed organizing target functions in query transformations directly
as key-value maps.  Together, Clojure helped improve the codebase by making it
more concise, intuitive and data-oriented, and thus more maintainable.

One may argue that similar could be achieved through Java 8 lambda expressions
and anonymous class, but we believe that such solution shall not reach
the same level of conciseness and intuitiveness.

\subsection{Concurrency}
The majority of concurrency handling is leveraged by the Aleph framework,
which efficiently utilize thread-pool to maximize \gls{cpu} utilization
and task switching for best \gls{io} performance.  Whilst running alone it can
serve up to tens of thousands or requests every second on a personal laptop,
it is not the only \gls{io}-intensize spot.  The way connections are made to
to persistence layers is also important.  As also shown in table~\ref{rdbpsql},
using a \gls{db} client with connection pool can greatly enhance
the performance.  The same could be said to the \gls{ipfs} client,
which currently does not implement any connection pool and seem to have
a relatively high latency compared to the \gls{db} counterpart.

Other concurrency mechanisms were also utilized, such as memoized \verb|future|
as a synchronization point to minimize re-execution of metadata extraction.
This was not a perfect solution though, while it take almost negligible time
to create the \verb|future| in listing~\ref{literal}, theoretically there still
exists a race condition.  More importantly, \verb|memoize| memoizes all results,
which occupy memory over time; a cache invalidation mechanism should be added.

\subsection{Query Polymorphism}
The use of a same query language for both dataset lookup and content extraction
has both advantages and downsides.  On the upside, an unified interface
reduces the complexity and is arguably easier to adopt.  Even in the development
of the core itself, the \gls{qast} allowed test cases to share queries
and regressions could be spotted from different results from a same query.

On the other hand, in a sense some flexibility was lost.  Some data formats
like \gls{xml} cannot have their structure expressed in a JSON schema.
Consequently, the data cannot be extracted without a workaround transforming
them to a \gls{json}-like format.  Such workaround is, though not pretty,
possible in theory.

\subsection{Use of Free Software}
Free (libre) software is used exclusively as dependencies and in the development
process.  The benefits of this went beyond making the resulting software
copyleft semantically.  For instance, schema inference was made after
studying Qri's source code\footnote{Although the implementation was not
directly based on the inspiration.} and several patches were made to a fork
of the \gls{ipfs} client for Java and redistributed specifically for
ComLake\footnote{\url{https://github.com/ComLake/java-ipfs-http-client}}.

That being said, some library such as JSON-java had to be avoided in order
to comply with \gls{agplv3}, or any other libre software license, as the
library's license has a non-free clause: ``The software shall be used for good,
not evil.''
